{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded user_filtered.csv.\n",
      "Successfully loaded anime_filtered.csv with selected columns.\n",
      "No duplicate anime_id entries found in anime_details.\n",
      "Number of anime after filtering: 16272\n",
      "Number of user ratings after filtering: 109179072\n",
      "Successfully merged filtered user_ratings with anime_details.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>anime_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>Name</th>\n",
       "      <th>sypnopsis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Basilisk: Kouga Ninpou Chou</td>\n",
       "      <td>For centuries, the Iga and Kouga ninja clans h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>6702</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Fairy Tail</td>\n",
       "      <td>In the mystical land of Fiore, magic exists as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>242</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Gokusen</td>\n",
       "      <td>Kumiko Yamaguchi is smart, enthusiastic, and r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4898</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Kuroshitsuji</td>\n",
       "      <td>Young Ciel Phantomhive is known as \"the Queen'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>10.0</td>\n",
       "      <td>One Piece</td>\n",
       "      <td>Gol D. Roger was known as the \"Pirate King,\" t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  anime_id  rating                         Name  \\\n",
       "0        0        67     9.0  Basilisk: Kouga Ninpou Chou   \n",
       "1        0      6702     7.0                   Fairy Tail   \n",
       "2        0       242    10.0                      Gokusen   \n",
       "3        0      4898     0.0                 Kuroshitsuji   \n",
       "4        0        21    10.0                    One Piece   \n",
       "\n",
       "                                           sypnopsis  \n",
       "0  For centuries, the Iga and Kouga ninja clans h...  \n",
       "1  In the mystical land of Fiore, magic exists as...  \n",
       "2  Kumiko Yamaguchi is smart, enthusiastic, and r...  \n",
       "3  Young Ciel Phantomhive is known as \"the Queen'...  \n",
       "4  Gol D. Roger was known as the \"Pirate King,\" t...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse Item-User Matrix Shape: (16272, 325770)\n",
      "Item Embeddings Shape: (16272, 100)\n",
      "Annoy index built successfully.\n"
     ]
    }
   ],
   "source": [
    "# Import Necessary Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import normalize\n",
    "from annoy import AnnoyIndex\n",
    "from scipy.sparse import csr_matrix\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ----------------------------\n",
    "# Step 1: Load user_filtered.csv\n",
    "# ----------------------------\n",
    "\n",
    "# Define data types for user_filtered.csv\n",
    "user_filtered_dtypes = {\n",
    "    'user_id': 'int32',\n",
    "    'anime_id': 'int32',\n",
    "    'rating': 'float32'\n",
    "}\n",
    "\n",
    "# Load user_filtered.csv\n",
    "try:\n",
    "    user_ratings = pd.read_csv(\n",
    "        'user-filtered.csv',\n",
    "        dtype=user_filtered_dtypes\n",
    "    )\n",
    "    print(\"Successfully loaded user_filtered.csv.\")\n",
    "except MemoryError:\n",
    "    print(\"MemoryError: Unable to load user_filtered.csv.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading user_filtered.csv: {e}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Step 2: Load anime_filtered.csv\n",
    "# ----------------------------\n",
    "\n",
    "# Define data types for anime_filtered.csv\n",
    "anime_filtered_dtypes = {\n",
    "    'anime_id': 'int32',\n",
    "    'Name': 'object',\n",
    "    'sypnopsis': 'object'\n",
    "}\n",
    "\n",
    "# Load only necessary columns to save memory\n",
    "try:\n",
    "    anime_details = pd.read_csv(\n",
    "        'anime-filtered.csv',\n",
    "        usecols=['anime_id', 'Name', 'sypnopsis'],\n",
    "        dtype=anime_filtered_dtypes\n",
    "    )\n",
    "    print(\"Successfully loaded anime_filtered.csv with selected columns.\")\n",
    "except MemoryError:\n",
    "    print(\"MemoryError: Unable to load anime_filtered.csv even after optimization.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading anime_filtered.csv: {e}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Step 3: Handle Duplicates in anime_details\n",
    "# ----------------------------\n",
    "\n",
    "# Check for duplicate anime_id in anime_details\n",
    "duplicate_anime_ids = anime_details[anime_details.duplicated('anime_id', keep=False)]\n",
    "if not duplicate_anime_ids.empty:\n",
    "    print(f\"Found {duplicate_anime_ids.shape[0]} duplicate anime_id entries in anime_details.\")\n",
    "    # Remove duplicates by keeping the first occurrence\n",
    "    anime_details = anime_details.drop_duplicates(subset='anime_id', keep='first')\n",
    "    print(\"Duplicates removed from anime_details.\")\n",
    "else:\n",
    "    print(\"No duplicate anime_id entries found in anime_details.\")\n",
    "\n",
    "# ----------------------------\n",
    "# Step 4: Filter Anime with Minimum Ratings\n",
    "# ----------------------------\n",
    "\n",
    "# Define the minimum number of ratings required\n",
    "min_ratings = 50\n",
    "\n",
    "# Calculate the number of ratings per anime\n",
    "anime_rating_counts = user_ratings['anime_id'].value_counts()\n",
    "\n",
    "# Identify anime_ids that meet the minimum rating threshold\n",
    "popular_anime = anime_rating_counts[anime_rating_counts >= min_ratings].index.tolist()\n",
    "\n",
    "# Filter user_ratings to include only popular anime\n",
    "user_ratings_filtered = user_ratings[user_ratings['anime_id'].isin(popular_anime)]\n",
    "\n",
    "print(f\"Number of anime after filtering: {len(popular_anime)}\")\n",
    "print(f\"Number of user ratings after filtering: {user_ratings_filtered.shape[0]}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Step 5: Merge Filtered DataFrames\n",
    "# ----------------------------\n",
    "\n",
    "# Merge the filtered user ratings with anime details\n",
    "ratings_with_anime_filtered = pd.merge(\n",
    "    user_ratings_filtered,\n",
    "    anime_details,\n",
    "    on='anime_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(\"Successfully merged filtered user_ratings with anime_details.\")\n",
    "display(ratings_with_anime_filtered.head())\n",
    "\n",
    "# ----------------------------\n",
    "# Step 6: Create Mappings\n",
    "# ----------------------------\n",
    "\n",
    "# Create unique lists of anime_ids\n",
    "anime_ids = ratings_with_anime_filtered['anime_id'].unique()\n",
    "\n",
    "# Create mappings from anime_id to index and vice versa\n",
    "anime_id_to_index = {anime_id: index for index, anime_id in enumerate(anime_ids)}\n",
    "index_to_anime_id = {index: anime_id for anime_id, index in anime_id_to_index.items()}\n",
    "\n",
    "# Create unique lists of user_ids\n",
    "user_ids = ratings_with_anime_filtered['user_id'].unique()\n",
    "\n",
    "# Create mapping from user_id to index\n",
    "user_id_to_index = {user_id: index for index, user_id in enumerate(user_ids)}\n",
    "\n",
    "# Map anime_ids and user_ids to indices\n",
    "anime_indices = ratings_with_anime_filtered['anime_id'].map(anime_id_to_index)\n",
    "user_indices = ratings_with_anime_filtered['user_id'].map(user_id_to_index)\n",
    "ratings = ratings_with_anime_filtered['rating'].values\n",
    "\n",
    "# ----------------------------\n",
    "# Step 7: Construct Sparse Item-User Matrix\n",
    "# ----------------------------\n",
    "\n",
    "# Create the sparse matrix (anime-user matrix)\n",
    "anime_user_matrix_sparse = csr_matrix((ratings, (anime_indices, user_indices)), \n",
    "                                      shape=(len(anime_ids), len(user_ids)))\n",
    "\n",
    "print(f\"Sparse Item-User Matrix Shape: {anime_user_matrix_sparse.shape}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Step 8: Dimensionality Reduction with Truncated SVD\n",
    "# ----------------------------\n",
    "\n",
    "# Define the number of latent factors (dimensions)\n",
    "n_factors = 100  # Adjust based on your system's capacity and desired accuracy\n",
    "\n",
    "# Initialize Truncated SVD\n",
    "svd = TruncatedSVD(n_components=n_factors, random_state=42)\n",
    "\n",
    "# Fit and transform the sparse item-user matrix to obtain item embeddings\n",
    "item_embeddings = svd.fit_transform(anime_user_matrix_sparse)\n",
    "\n",
    "print(f\"Item Embeddings Shape: {item_embeddings.shape}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Step 9: Normalize Embeddings\n",
    "# ----------------------------\n",
    "\n",
    "# Normalize embeddings for cosine similarity\n",
    "item_embeddings_normalized = normalize(item_embeddings, norm='l2', axis=1)\n",
    "\n",
    "# ----------------------------\n",
    "# Step 10: Build the Annoy Index\n",
    "# ----------------------------\n",
    "\n",
    "# Define the number of trees for Annoy\n",
    "n_trees = 10  # More trees give higher accuracy but take longer to build\n",
    "\n",
    "# Initialize Annoy Index\n",
    "annoy_index = AnnoyIndex(n_factors, 'angular')  # 'angular' is suitable for cosine similarity\n",
    "\n",
    "# Add item embeddings to Annoy index\n",
    "for i in range(item_embeddings_normalized.shape[0]):\n",
    "    annoy_index.add_item(i, item_embeddings_normalized[i])\n",
    "\n",
    "# Build the index\n",
    "annoy_index.build(n_trees)\n",
    "\n",
    "print(\"Annoy index built successfully.\")\n",
    "\n",
    "# ----------------------------\n",
    "# Step 11: Define the Recommendation Function\n",
    "# ----------------------------\n",
    "\n",
    "def recommend_anime_annoy(anime_title, anime_details, annoy_index, \n",
    "                         anime_id_to_index, index_to_anime_id, \n",
    "                         item_embeddings_normalized, n_neighbors=5, n_recommendations=5):\n",
    "    \"\"\"\n",
    "    Recommend anime based on a given anime title using Annoy.\n",
    "    \n",
    "    Parameters:\n",
    "    - anime_title (str): The title of the anime to base recommendations on.\n",
    "    - anime_details (DataFrame): The anime details dataset.\n",
    "    - annoy_index (AnnoyIndex): The Annoy index containing item embeddings.\n",
    "    - anime_id_to_index (dict): Mapping from anime_id to matrix index.\n",
    "    - index_to_anime_id (dict): Mapping from matrix index to anime_id.\n",
    "    - item_embeddings_normalized (ndarray): The normalized dense item embeddings.\n",
    "    - n_neighbors (int): Number of similar anime to consider.\n",
    "    - n_recommendations (int): Number of anime to recommend.\n",
    "    \n",
    "    Returns:\n",
    "    - recommendations (DataFrame): Recommended anime with synopsis.\n",
    "    \"\"\"\n",
    "    # Find the anime_id based on the title\n",
    "    anime_subset = anime_details[anime_details['Name'].str.lower() == anime_title.lower()]\n",
    "    \n",
    "    if anime_subset.empty:\n",
    "        print(\"Anime title not found in the dataset.\")\n",
    "        return\n",
    "    \n",
    "    anime_id = anime_subset['anime_id'].values[0]\n",
    "    anime_idx = anime_id_to_index.get(anime_id, None)\n",
    "    \n",
    "    if anime_idx is None:\n",
    "        print(\"Anime index not found.\")\n",
    "        return\n",
    "    \n",
    "    # Find similar anime indices using Annoy\n",
    "    similar_anime_indices = annoy_index.get_nns_by_item(anime_idx, n_neighbors + 1)[1:]  # Exclude the anime itself\n",
    "    \n",
    "    # Map indices back to anime_ids\n",
    "    similar_anime_ids = [index_to_anime_id[idx] for idx in similar_anime_indices]\n",
    "    \n",
    "    # Fetch anime details for similar anime\n",
    "    recommended_anime = anime_details[anime_details['anime_id'].isin(similar_anime_ids)][['Name', 'sypnopsis']]\n",
    "    \n",
    "    return recommended_anime.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Recommendations based on 'Toriko':\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>sypnopsis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eyeshield 21</td>\n",
       "      <td>Sena is like any other shy kid starting high s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Grappler Baki (TV)</td>\n",
       "      <td>Ever since he was born, Baki Hanma has always ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Katekyo Hitman Reborn!</td>\n",
       "      <td>There is no putting it lightly—Tsunayoshi Sawa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cardfight!! Vanguard</td>\n",
       "      <td>Cardfight!! Vanguard features a world where th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kingdom</td>\n",
       "      <td>China’s Warring States period, a raging dragon...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Name                                          sypnopsis\n",
       "0            Eyeshield 21  Sena is like any other shy kid starting high s...\n",
       "1      Grappler Baki (TV)  Ever since he was born, Baki Hanma has always ...\n",
       "2  Katekyo Hitman Reborn!  There is no putting it lightly—Tsunayoshi Sawa...\n",
       "3    Cardfight!! Vanguard  Cardfight!! Vanguard features a world where th...\n",
       "4                 Kingdom  China’s Warring States period, a raging dragon..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example Usage\n",
    "favorite_anime_title = \"Toriko\"  # Replace with the desired anime title\n",
    "\n",
    "recommended_anime = recommend_anime_annoy(\n",
    "    anime_title=favorite_anime_title,\n",
    "    anime_details=anime_details,\n",
    "    annoy_index=annoy_index,\n",
    "    anime_id_to_index=anime_id_to_index,\n",
    "    index_to_anime_id=index_to_anime_id,\n",
    "    item_embeddings_normalized=item_embeddings_normalized,\n",
    "    n_neighbors=5,\n",
    "    n_recommendations=5\n",
    ")\n",
    "\n",
    "print(f\"Top 5 Recommendations based on '{favorite_anime_title}':\\n\")\n",
    "display(recommended_anime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded user_filtered.csv.\n",
      "Successfully loaded anime_filtered.csv with selected columns.\n",
      "No duplicate anime_id entries found in anime_details.\n",
      "Number of anime after filtering: 16272\n",
      "Number of user ratings after filtering: 109179072\n",
      "Successfully merged filtered user_ratings with anime_details.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>anime_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>Name</th>\n",
       "      <th>sypnopsis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Basilisk: Kouga Ninpou Chou</td>\n",
       "      <td>For centuries, the Iga and Kouga ninja clans h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>6702</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Fairy Tail</td>\n",
       "      <td>In the mystical land of Fiore, magic exists as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>242</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Gokusen</td>\n",
       "      <td>Kumiko Yamaguchi is smart, enthusiastic, and r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4898</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Kuroshitsuji</td>\n",
       "      <td>Young Ciel Phantomhive is known as \"the Queen'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>10.0</td>\n",
       "      <td>One Piece</td>\n",
       "      <td>Gol D. Roger was known as the \"Pirate King,\" t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  anime_id  rating                         Name  \\\n",
       "0        0        67     9.0  Basilisk: Kouga Ninpou Chou   \n",
       "1        0      6702     7.0                   Fairy Tail   \n",
       "2        0       242    10.0                      Gokusen   \n",
       "3        0      4898     0.0                 Kuroshitsuji   \n",
       "4        0        21    10.0                    One Piece   \n",
       "\n",
       "                                           sypnopsis  \n",
       "0  For centuries, the Iga and Kouga ninja clans h...  \n",
       "1  In the mystical land of Fiore, magic exists as...  \n",
       "2  Kumiko Yamaguchi is smart, enthusiastic, and r...  \n",
       "3  Young Ciel Phantomhive is known as \"the Queen'...  \n",
       "4  Gol D. Roger was known as the \"Pirate King,\" t...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse User-Item Matrix Shape: (325770, 16272)\n",
      "User Embeddings Shape: (325770, 100)\n",
      "Annoy index built successfully.\n",
      "Top 5 Recommendations for User ID 1:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>sypnopsis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DearS</td>\n",
       "      <td>One year ago, a UFO containing 150 aliens cras...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>El Hazard: The Wanderers</td>\n",
       "      <td>High school science-whiz Makoto Mizuhara is wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fruits Basket</td>\n",
       "      <td>fter the accident in which she lost her mother...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gensoumaden Saiyuuki</td>\n",
       "      <td>any years ago, humans and demons lived in harm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sakura Taisen</td>\n",
       "      <td>Sakura travels to the capital with aspirations...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Name                                          sypnopsis\n",
       "0                     DearS  One year ago, a UFO containing 150 aliens cras...\n",
       "1  El Hazard: The Wanderers  High school science-whiz Makoto Mizuhara is wo...\n",
       "2             Fruits Basket  fter the accident in which she lost her mother...\n",
       "3      Gensoumaden Saiyuuki  any years ago, humans and demons lived in harm...\n",
       "4             Sakura Taisen  Sakura travels to the capital with aspirations..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import Necessary Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import normalize\n",
    "from annoy import AnnoyIndex\n",
    "from scipy.sparse import csr_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ----------------------------\n",
    "# Step 1: Load user_filtered.csv\n",
    "# ----------------------------\n",
    "\n",
    "# Define data types for user_filtered.csv\n",
    "user_filtered_dtypes = {\n",
    "    'user_id': 'int32',\n",
    "    'anime_id': 'int32',\n",
    "    'rating': 'float32'\n",
    "}\n",
    "\n",
    "# Load user_filtered.csv\n",
    "try:\n",
    "    user_ratings = pd.read_csv(\n",
    "        'user-filtered.csv',\n",
    "        dtype=user_filtered_dtypes\n",
    "    )\n",
    "    print(\"Successfully loaded user_filtered.csv.\")\n",
    "except MemoryError:\n",
    "    print(\"MemoryError: Unable to load user_filtered.csv.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading user_filtered.csv: {e}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Step 2: Load anime_filtered.csv\n",
    "# ----------------------------\n",
    "\n",
    "# Define data types for anime_filtered.csv\n",
    "anime_filtered_dtypes = {\n",
    "    'anime_id': 'int32',\n",
    "    'Name': 'object',\n",
    "    'sypnopsis': 'object'\n",
    "}\n",
    "\n",
    "# Load only necessary columns to save memory\n",
    "try:\n",
    "    anime_details = pd.read_csv(\n",
    "        'anime-filtered.csv',\n",
    "        usecols=['anime_id', 'Name', 'sypnopsis'],\n",
    "        dtype=anime_filtered_dtypes\n",
    "    )\n",
    "    print(\"Successfully loaded anime_filtered.csv with selected columns.\")\n",
    "except MemoryError:\n",
    "    print(\"MemoryError: Unable to load anime_filtered.csv even after optimization.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading anime_filtered.csv: {e}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Step 3: Handle Duplicates in anime_details\n",
    "# ----------------------------\n",
    "\n",
    "# Check for duplicate anime_id in anime_details\n",
    "duplicate_anime_ids = anime_details[anime_details.duplicated('anime_id', keep=False)]\n",
    "if not duplicate_anime_ids.empty:\n",
    "    print(f\"Found {duplicate_anime_ids.shape[0]} duplicate anime_id entries in anime_details.\")\n",
    "    # Remove duplicates by keeping the first occurrence\n",
    "    anime_details = anime_details.drop_duplicates(subset='anime_id', keep='first')\n",
    "    print(\"Duplicates removed from anime_details.\")\n",
    "else:\n",
    "    print(\"No duplicate anime_id entries found in anime_details.\")\n",
    "\n",
    "# ----------------------------\n",
    "# Step 4: Filter Anime with Minimum Ratings\n",
    "# ----------------------------\n",
    "\n",
    "# Define the minimum number of ratings required\n",
    "min_ratings = 50\n",
    "\n",
    "# Calculate the number of ratings per anime\n",
    "anime_rating_counts = user_ratings['anime_id'].value_counts()\n",
    "\n",
    "# Identify anime_ids that meet the minimum rating threshold\n",
    "popular_anime = anime_rating_counts[anime_rating_counts >= min_ratings].index.tolist()\n",
    "\n",
    "# Filter user_ratings to include only popular anime\n",
    "user_ratings_filtered = user_ratings[user_ratings['anime_id'].isin(popular_anime)]\n",
    "\n",
    "print(f\"Number of anime after filtering: {len(popular_anime)}\")\n",
    "print(f\"Number of user ratings after filtering: {user_ratings_filtered.shape[0]}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Step 5: Merge Filtered DataFrames\n",
    "# ----------------------------\n",
    "\n",
    "# Merge the filtered user ratings with anime details\n",
    "ratings_with_anime_filtered = pd.merge(\n",
    "    user_ratings_filtered,\n",
    "    anime_details,\n",
    "    on='anime_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(\"Successfully merged filtered user_ratings with anime_details.\")\n",
    "display(ratings_with_anime_filtered.head())\n",
    "\n",
    "# ----------------------------\n",
    "# Step 6: Create Mappings\n",
    "# ----------------------------\n",
    "\n",
    "# Create unique lists of user_ids and anime_ids\n",
    "user_ids = ratings_with_anime_filtered['user_id'].unique()\n",
    "anime_ids = ratings_with_anime_filtered['anime_id'].unique()\n",
    "\n",
    "# Create mappings from ids to indices\n",
    "user_id_to_index = {user_id: index for index, user_id in enumerate(user_ids)}\n",
    "anime_id_to_index = {anime_id: index for index, anime_id in enumerate(anime_ids)}\n",
    "index_to_anime_id = {index: anime_id for anime_id, index in anime_id_to_index.items()}\n",
    "\n",
    "# Map user_ids and anime_ids to indices\n",
    "user_indices = ratings_with_anime_filtered['user_id'].map(user_id_to_index)\n",
    "anime_indices = ratings_with_anime_filtered['anime_id'].map(anime_id_to_index)\n",
    "ratings = ratings_with_anime_filtered['rating'].values\n",
    "\n",
    "# ----------------------------\n",
    "# Step 7: Construct Sparse User-Item Matrix\n",
    "# ----------------------------\n",
    "\n",
    "# Create the sparse matrix\n",
    "user_item_matrix_sparse = csr_matrix((ratings, (user_indices, anime_indices)), \n",
    "                                     shape=(len(user_ids), len(anime_ids)))\n",
    "\n",
    "print(f\"Sparse User-Item Matrix Shape: {user_item_matrix_sparse.shape}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Step 8: Dimensionality Reduction with Truncated SVD\n",
    "# ----------------------------\n",
    "\n",
    "# Define the number of latent factors (dimensions)\n",
    "n_factors = 100  # Adjust based on your system's capacity and desired accuracy\n",
    "\n",
    "# Initialize Truncated SVD\n",
    "svd = TruncatedSVD(n_components=n_factors, random_state=42)\n",
    "\n",
    "# Fit and transform the sparse user-item matrix to obtain user embeddings\n",
    "user_embeddings = svd.fit_transform(user_item_matrix_sparse)\n",
    "\n",
    "print(f\"User Embeddings Shape: {user_embeddings.shape}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Step 9: Normalize Embeddings\n",
    "# ----------------------------\n",
    "\n",
    "# Normalize embeddings for cosine similarity\n",
    "user_embeddings_normalized = normalize(user_embeddings, norm='l2', axis=1)\n",
    "\n",
    "# ----------------------------\n",
    "# Step 10: Build the Annoy Index\n",
    "# ----------------------------\n",
    "\n",
    "# Define the number of trees for Annoy\n",
    "n_trees = 10  # More trees give higher accuracy but take longer to build\n",
    "\n",
    "# Initialize Annoy Index\n",
    "annoy_index = AnnoyIndex(n_factors, 'angular')  # 'angular' is suitable for cosine similarity\n",
    "\n",
    "# Add user embeddings to Annoy index\n",
    "for i in range(user_embeddings_normalized.shape[0]):\n",
    "    annoy_index.add_item(i, user_embeddings_normalized[i])\n",
    "\n",
    "# Build the index\n",
    "annoy_index.build(n_trees)\n",
    "\n",
    "print(\"Annoy index built successfully.\")\n",
    "\n",
    "# ----------------------------\n",
    "# Step 11: Define the Recommendation Function\n",
    "# ----------------------------\n",
    "\n",
    "def recommend_anime_annoy(user_id, user_item_matrix, anime_details, annoy_index, \n",
    "                         user_id_to_index, anime_id_to_index, \n",
    "                         index_to_anime_id, user_embeddings_normalized, n_neighbors=5, n_recommendations=5):\n",
    "    \"\"\"\n",
    "    Recommend anime to a user based on Annoy.\n",
    "\n",
    "    Parameters:\n",
    "    - user_id (int): The ID of the user to make recommendations for.\n",
    "    - user_item_matrix (csr_matrix): The user-item rating matrix.\n",
    "    - anime_details (DataFrame): The anime details dataset.\n",
    "    - annoy_index (AnnoyIndex): The Annoy index containing user embeddings.\n",
    "    - user_id_to_index (dict): Mapping from user_id to matrix index.\n",
    "    - anime_id_to_index (dict): Mapping from anime_id to matrix index.\n",
    "    - index_to_anime_id (dict): Mapping from matrix index to anime_id.\n",
    "    - user_embeddings_normalized (ndarray): The normalized dense user embeddings.\n",
    "    - n_neighbors (int): Number of similar users to consider.\n",
    "    - n_recommendations (int): Number of anime to recommend.\n",
    "\n",
    "    Returns:\n",
    "    - recommendations (DataFrame): Recommended anime with synopsis.\n",
    "    \"\"\"\n",
    "    if user_id not in user_id_to_index:\n",
    "        print(\"User ID not found in the dataset.\")\n",
    "        return\n",
    "\n",
    "    # Get the index of the user\n",
    "    user_idx = user_id_to_index[user_id]\n",
    "\n",
    "    # Find similar users\n",
    "    similar_users_indices = annoy_index.get_nns_by_item(user_idx, n_neighbors + 1)[1:]  # Exclude the user itself\n",
    "\n",
    "    # Aggregate ratings from similar users\n",
    "    similar_users_ratings = user_item_matrix[similar_users_indices]\n",
    "\n",
    "    # Compute the mean ratings for each anime\n",
    "    mean_ratings = similar_users_ratings.mean(axis=0).A1  # Convert to 1D array\n",
    "\n",
    "    # Convert to a Series for easier manipulation\n",
    "    mean_ratings_series = pd.Series(mean_ratings, index=index_to_anime_id.keys())\n",
    "\n",
    "    # Get anime_ids already rated by the user\n",
    "    user_ratings = user_item_matrix[user_idx]\n",
    "    user_rated_anime = user_ratings.nonzero()[1]\n",
    "    user_rated_anime_ids = [index_to_anime_id[idx] for idx in user_rated_anime]\n",
    "\n",
    "    # Exclude anime already rated by the user\n",
    "    recommendations = mean_ratings_series.drop(user_rated_anime_ids, errors='ignore')\n",
    "\n",
    "    # Get top N recommendations\n",
    "    top_recommendations = recommendations.sort_values(ascending=False).head(n_recommendations).index.tolist()\n",
    "\n",
    "    # Fetch anime details\n",
    "    recommended_anime = anime_details[anime_details['anime_id'].isin(top_recommendations)][['Name', 'sypnopsis']]\n",
    "\n",
    "    return recommended_anime.reset_index(drop=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Step 12: Generate Recommendations for a Specific User\n",
    "# ----------------------------\n",
    "\n",
    "# Example Usage\n",
    "user_id_input = 1  # Replace with the desired user_id\n",
    "\n",
    "recommended_anime_annoy = recommend_anime_annoy(\n",
    "    user_id=user_id_input,\n",
    "    user_item_matrix=user_item_matrix_sparse,\n",
    "    anime_details=anime_details,\n",
    "    annoy_index=annoy_index,\n",
    "    user_id_to_index=user_id_to_index,\n",
    "    anime_id_to_index=anime_id_to_index,\n",
    "    index_to_anime_id=index_to_anime_id,\n",
    "    user_embeddings_normalized=user_embeddings_normalized,\n",
    "    n_neighbors=5,\n",
    "    n_recommendations=5\n",
    ")\n",
    "\n",
    "print(f\"Top 5 Recommendations for User ID {user_id_input}:\\n\")\n",
    "display(recommended_anime_annoy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " With Confidence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import normalize\n",
    "from annoy import AnnoyIndex\n",
    "from scipy.sparse import csr_matrix\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load user ratings\n",
    "user_ratings = pd.read_csv('user-filtered.csv')  # Columns: user_id, anime_id, rating\n",
    "\n",
    "# Load anime details\n",
    "anime_details = pd.read_csv('anime-filtered.csv')  # Columns: anime_id, Name, sypnopsis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 87379797\n",
      "Test set size: 21844950\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define test size\n",
    "test_size = 0.2  # 20% for testing\n",
    "\n",
    "# Perform the split\n",
    "train, test = train_test_split(user_ratings, test_size=test_size, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {train.shape[0]}\")\n",
    "print(f\"Test set size: {test.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Test set size: 21844950\n"
     ]
    }
   ],
   "source": [
    "# Retain only those anime_ids in the test set that are present in the training set\n",
    "test = test[test['anime_id'].isin(train['anime_id'].unique())]\n",
    "\n",
    "print(f\"Adjusted Test set size: {test.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of common users: 318057\n",
      "Training set size after user filtering: 87353702\n",
      "Test set size after user filtering: 21844119\n"
     ]
    }
   ],
   "source": [
    "# Find users present in both train and test\n",
    "common_users = set(train['user_id']).intersection(set(test['user_id']))\n",
    "\n",
    "# Retain only interactions from common users\n",
    "train = train[train['user_id'].isin(common_users)]\n",
    "test = test[test['user_id'].isin(common_users)]\n",
    "\n",
    "print(f\"Number of common users: {len(common_users)}\")\n",
    "print(f\"Training set size after user filtering: {train.shape[0]}\")\n",
    "print(f\"Test set size after user filtering: {test.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.9.2-cp39-cp39-win_amd64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.0-cp39-cp39-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.54.1-cp39-cp39-win_amd64.whl.metadata (167 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.7-cp39-cp39-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\shiba\\anaconda3\\envs\\anime_rec_sys\\lib\\site-packages (from matplotlib) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shiba\\anaconda3\\envs\\anime_rec_sys\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-10.4.0-cp39-cp39-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.1.4-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\shiba\\anaconda3\\envs\\anime_rec_sys\\lib\\site-packages (from matplotlib) (2.9.0)\n",
      "Collecting importlib-resources>=3.2.0 (from matplotlib)\n",
      "  Downloading importlib_resources-6.4.5-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\shiba\\anaconda3\\envs\\anime_rec_sys\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.20.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shiba\\anaconda3\\envs\\anime_rec_sys\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.9.2-cp39-cp39-win_amd64.whl (7.8 MB)\n",
      "   ---------------------------------------- 0.0/7.8 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 2.6/7.8 MB 18.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.8/7.8 MB 20.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.8/7.8 MB 18.6 MB/s eta 0:00:00\n",
      "Downloading contourpy-1.3.0-cp39-cp39-win_amd64.whl (211 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.54.1-cp39-cp39-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.2/2.2 MB 17.7 MB/s eta 0:00:00\n",
      "Downloading importlib_resources-6.4.5-py3-none-any.whl (36 kB)\n",
      "Downloading kiwisolver-1.4.7-cp39-cp39-win_amd64.whl (55 kB)\n",
      "Downloading pillow-10.4.0-cp39-cp39-win_amd64.whl (2.6 MB)\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.6/2.6 MB 21.0 MB/s eta 0:00:00\n",
      "Using cached pyparsing-3.1.4-py3-none-any.whl (104 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, importlib-resources, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.0 cycler-0.12.1 fonttools-4.54.1 importlib-resources-6.4.5 kiwisolver-1.4.7 matplotlib-3.9.2 pillow-10.4.0 pyparsing-3.1.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define relevant threshold\n",
    "relevant_threshold = 4\n",
    "test_relevant = test[test['rating'] >= relevant_threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create unique lists of anime_ids and user_ids\n",
    "anime_ids = train['anime_id'].unique()\n",
    "user_ids = train['user_id'].unique()\n",
    "\n",
    "# Create mappings from anime_id to index and vice versa\n",
    "anime_id_to_index = {anime_id: idx for idx, anime_id in enumerate(anime_ids)}\n",
    "index_to_anime_id = {idx: anime_id for anime_id, idx in anime_id_to_index.items()}\n",
    "\n",
    "# Create mapping from user_id to index\n",
    "user_id_to_index = {user_id: idx for idx, user_id in enumerate(user_ids)}\n",
    "\n",
    "# Map anime_ids and user_ids to indices\n",
    "anime_indices_train = train['anime_id'].map(anime_id_to_index)\n",
    "user_indices_train = train['user_id'].map(user_id_to_index)\n",
    "ratings_train = train['rating'].values\n",
    "\n",
    "# Create the sparse matrix (anime-user matrix)\n",
    "num_anime = len(anime_ids)\n",
    "num_users = len(user_ids)\n",
    "anime_user_matrix_sparse = csr_matrix((ratings_train, (anime_indices_train, user_indices_train)),\n",
    "                                      shape=(num_anime, num_users))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of latent factors (dimensions)\n",
    "n_factors = 100  # Adjust based on your system's capacity and desired accuracy\n",
    "\n",
    "# Initialize Truncated SVD\n",
    "svd = TruncatedSVD(n_components=n_factors, random_state=42)\n",
    "\n",
    "# Fit and transform the sparse anime-user matrix to obtain item embeddings\n",
    "item_embeddings = svd.fit_transform(anime_user_matrix_sparse)\n",
    "\n",
    "# Normalize embeddings for cosine similarity\n",
    "item_embeddings_normalized = normalize(item_embeddings, norm='l2', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annoy index built successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define the number of trees for Annoy\n",
    "n_trees = 10  # More trees give higher accuracy but take longer to build\n",
    "\n",
    "# Initialize Annoy Index\n",
    "annoy_index = AnnoyIndex(n_factors, 'angular')  # 'angular' is suitable for cosine similarity\n",
    "\n",
    "# Add item embeddings to Annoy index\n",
    "for i in range(item_embeddings_normalized.shape[0]):\n",
    "    annoy_index.add_item(i, item_embeddings_normalized[i])\n",
    "\n",
    "# Build the index\n",
    "annoy_index.build(n_trees)\n",
    "\n",
    "print(\"Annoy index built successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_anime_with_confidence(anime_title, anime_details, annoy_index, \n",
    "                                    anime_id_to_index, index_to_anime_id, \n",
    "                                    item_embeddings_normalized, n_neighbors=5, n_recommendations=5):\n",
    "    \"\"\"\n",
    "    Recommend anime based on a given anime title using Annoy, along with confidence scores.\n",
    "    \n",
    "    Parameters:\n",
    "    - anime_title (str): The title of the anime to base recommendations on.\n",
    "    - anime_details (DataFrame): The anime details dataset.\n",
    "    - annoy_index (AnnoyIndex): The Annoy index containing item embeddings.\n",
    "    - anime_id_to_index (dict): Mapping from anime_id to matrix index.\n",
    "    - index_to_anime_id (dict): Mapping from matrix index to anime_id.\n",
    "    - item_embeddings_normalized (ndarray): The normalized dense item embeddings.\n",
    "    - n_neighbors (int): Number of similar anime to consider.\n",
    "    - n_recommendations (int): Number of anime to recommend.\n",
    "    \n",
    "    Returns:\n",
    "    - recommendations (DataFrame): Recommended anime with synopsis and confidence scores.\n",
    "    \"\"\"\n",
    "    # Find the anime_id based on the title\n",
    "    anime_subset = anime_details[anime_details['Name'].str.lower() == anime_title.lower()]\n",
    "    \n",
    "    if anime_subset.empty:\n",
    "        print(\"Anime title not found in the dataset.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    anime_id = anime_subset['anime_id'].values[0]\n",
    "    anime_idx = anime_id_to_index.get(anime_id, None)\n",
    "    \n",
    "    if anime_idx is None:\n",
    "        print(\"Anime index not found.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Find similar anime indices using Annoy\n",
    "    similar_anime_indices, distances = annoy_index.get_nns_by_item(\n",
    "        anime_idx, \n",
    "        n_neighbors + 1,  # +1 to exclude the anime itself\n",
    "        include_distances=True\n",
    "    )\n",
    "    \n",
    "    # Exclude the anime itself\n",
    "    similar_anime_indices = similar_anime_indices[1:]\n",
    "    distances = distances[1:]\n",
    "    \n",
    "    # Convert angular distances back to cosine similarity\n",
    "    # Cosine similarity = 1 - (distance^2 / 2)\n",
    "    cosine_similarities = 1 - (np.array(distances)**2) / 2\n",
    "    \n",
    "    # Map indices back to anime_ids\n",
    "    similar_anime_ids = [index_to_anime_id[idx] for idx in similar_anime_indices]\n",
    "    \n",
    "    # Fetch anime details for similar anime\n",
    "    recommended_anime = anime_details[anime_details['anime_id'].isin(similar_anime_ids)].copy()\n",
    "    \n",
    "    # Add confidence scores\n",
    "    recommended_anime['Confidence_Score'] = cosine_similarities\n",
    "    \n",
    "    # Sort by confidence score descending\n",
    "    recommended_anime = recommended_anime.sort_values(by='Confidence_Score', ascending=False)\n",
    "    \n",
    "    # Select top N recommendations\n",
    "    recommended_anime = recommended_anime.head(n_recommendations)\n",
    "    \n",
    "    return recommended_anime[['Name', 'sypnopsis', 'Confidence_Score']].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Recommendations based on 'Toriko':\n",
      "\n",
      "                        Name  \\\n",
      "0               Eyeshield 21   \n",
      "1     Katekyo Hitman Reborn!   \n",
      "2                  Beelzebub   \n",
      "3  Phi Brain: Kami no Puzzle   \n",
      "4                    Kingdom   \n",
      "\n",
      "                                           sypnopsis  Confidence_Score  \n",
      "0  Sena is like any other shy kid starting high s...          0.801258  \n",
      "1  There is no putting it lightly—Tsunayoshi Sawa...          0.794274  \n",
      "2  Ishiyama High is a school populated entirely b...          0.788396  \n",
      "3  Kaito Daimon would be a completely average hig...          0.781528  \n",
      "4  China’s Warring States period, a raging dragon...          0.773314  \n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "favorite_anime_title = \"Toriko\"  # Replace with an existing title in your dataset\n",
    "\n",
    "recommended_anime = recommend_anime_with_confidence(\n",
    "    anime_title=favorite_anime_title,\n",
    "    anime_details=anime_details,\n",
    "    annoy_index=annoy_index,\n",
    "    anime_id_to_index=anime_id_to_index,\n",
    "    index_to_anime_id=index_to_anime_id,\n",
    "    item_embeddings_normalized=item_embeddings_normalized,\n",
    "    n_neighbors=5,\n",
    "    n_recommendations=5\n",
    ")\n",
    "\n",
    "print(f\"Top 5 Recommendations based on '{favorite_anime_title}':\\n\")\n",
    "print(recommended_anime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anime_rec_sys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
